{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import parser\n",
    "import spacy \n",
    "from spacy.matcher import PhraseMatcher \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# terms would be each list in the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open ('2020_july_lexiconV4_7.json') as f1: # import the latest lexicon\n",
    "    dic1 = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the dataset \n",
    "\n",
    "file1 = '2020_FB_Pres_Ads_500_Topic_Gold_v1_test.xlsx'\n",
    "file2 = '2020_FB_Pres_Posts_500_Topic_GOLD_v1_test.xlsx'\n",
    "file3 = '2020_TW_Pres_Posts_500_Topic_Gold_v1_test.xlsx'\n",
    "dirs = 'C:\\\\Users\\\\wangtao\\\\Documents\\\\Illumination\\\\data_set(2020july)\\\\'\n",
    "\n",
    "# df1 = pd.read_excel(dirs+file3)\n",
    "\n",
    "df1 = pd.read_csv('FBtest_1500.csv') # here we import combined the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column that contains the text should be uniformly renamed as \"text\" \n",
    "df1.rename(columns={'ad_creative_body':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset \n",
    "if df1['text'].isnull().sum():\n",
    "    print (df1['text'].isnull().sum())\n",
    "    df1 = df1[df1['text'].notnull()]\n",
    "    df1.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Gold Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1494</td>\n",
       "      <td>Yo invertiría en infraestructura para que pers...</td>\n",
       "      <td>no topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>“During his Wisconsin out-reach, it became cle...</td>\n",
       "      <td>no topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>Another endorsement coming out of Iowa today! ...</td>\n",
       "      <td>no topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>Wow! Thanks everyone for supporting Help Meado...</td>\n",
       "      <td>no topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>Join us at the Spurs vs. Lakers on November 25...</td>\n",
       "      <td>no topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text Gold Label\n",
       "1494  Yo invertiría en infraestructura para que pers...   no topic\n",
       "1495  “During his Wisconsin out-reach, it became cle...   no topic\n",
       "1496  Another endorsement coming out of Iowa today! ...   no topic\n",
       "1497  Wow! Thanks everyone for supporting Help Meado...   no topic\n",
       "1498  Join us at the Spurs vs. Lakers on November 25...   no topic"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df1.tail()  # check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that filter the urls and symbols in the text \n",
    "\n",
    "def filter_text(x):\n",
    "    url = 'http[s]?://\\S+'\n",
    "    x = re.sub(url,'',x)\n",
    "    x = re.sub(\"[^\\w\\s]\",' ',x) # filter symbols\n",
    "    x = re.sub(\"\\s+\",' ',x)\n",
    "    \n",
    "    ls=[w.lower() for w in x.split()] \n",
    "    \n",
    "    return ' '.join(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'] = df1['text'].astype(str).apply(lambda x: filter_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function that find the lexicon words in the text\n",
    "def find_words(x,lexicon):\n",
    "    topics= lexicon.keys()  \n",
    "    doc = nlp(x) # nlp() is spaCy 2.2 English language model \n",
    "    words= []\n",
    "    for t in topics:\n",
    "        terms= lexicon[t]\n",
    "        patterns = [nlp.make_doc(text) for text in terms] \n",
    "        matcher.add(\"TerminologyList\", None, *patterns) # spaCy2.2 phrase matcher\n",
    "        matches = matcher(doc)\n",
    "        for match_id, start,end in matches:\n",
    "            span = doc[start:end]\n",
    "            words.append(span.text)\n",
    "    if words:\n",
    "        words = list(set(words))\n",
    "        return ','.join(words)\n",
    "    else:\n",
    "        return('no words')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging the topic in each message\n",
    "def find_topic(x,lexicon):\n",
    "    topics= lexicon.keys()    \n",
    "    if x=='no words':\n",
    "        return ''    \n",
    "    if x != 'no words': \n",
    "        words = x.split(',')\n",
    "        labels = []        \n",
    "        for t in topics:            \n",
    "            terms = lexicon[t]\n",
    "            if set(words)&set(terms):\n",
    "                labels.append(t)                \n",
    "                #l = sorted(labels)        \n",
    "        return  ','.join(sorted(labels))\n",
    "                \n",
    "        #return ','.join(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1['words'] = df1['text'].astype(str).apply(lambda x: find_words(x,dic1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['m_label'] = df1['words'].apply(lambda x: find_topic(x,dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['m_label'] = df1['m_label'].apply(lambda x: 'no topic' if x=='' else x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
